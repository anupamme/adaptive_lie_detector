"""Analyze question types and their diagnostic value."""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import List, Dict, Tuple, Any
import numpy as np
from collections import defaultdict

from src.adaptive_system import InterrogationResult, InterrogationStatus

try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("Warning: matplotlib not available, plots will be skipped")


def categorize_questions(questions: List[str]) -> Dict[str, int]:
    """
    Categorize questions by type.

    Categories:
    - detail_probe: Asking for specific details
    - consistency_check: Checking consistency with previous answers
    - knowledge_test: Testing factual knowledge
    - elaboration_request: Asking to expand on previous answer
    - other: Doesn't fit other categories

    Args:
        questions: List of questions to categorize

    Returns:
        Dictionary mapping category to count
    """
    categories = {
        "detail_probe": 0,
        "consistency_check": 0,
        "knowledge_test": 0,
        "elaboration_request": 0,
        "other": 0
    }

    # Keywords for each category
    detail_keywords = ["specific", "exactly", "when", "where", "how", "what time",
                       "which", "describe", "details", "particular"]
    consistency_keywords = ["earlier", "mentioned", "said", "previous", "before",
                           "consistent", "compared to", "contradiction"]
    knowledge_keywords = ["explain", "what is", "what are", "define", "how does",
                         "why", "tell me about", "what do you know"]
    elaboration_keywords = ["more about", "elaborate", "expand", "tell me more",
                           "can you explain", "go on", "continue"]

    for question in questions:
        q_lower = question.lower()

        # Check each category
        if any(keyword in q_lower for keyword in consistency_keywords):
            categories["consistency_check"] += 1
        elif any(keyword in q_lower for keyword in detail_keywords):
            categories["detail_probe"] += 1
        elif any(keyword in q_lower for keyword in elaboration_keywords):
            categories["elaboration_request"] += 1
        elif any(keyword in q_lower for keyword in knowledge_keywords):
            categories["knowledge_test"] += 1
        else:
            categories["other"] += 1

    return categories


def analyze_diagnostic_value(
    results: List[InterrogationResult]
) -> Dict[str, Dict]:
    """
    Analyze which question types lead to biggest confidence changes.

    Args:
        results: List of interrogation results

    Returns:
        Dictionary with diagnostic value statistics per category
    """
    # Extract questions and confidence changes from results
    category_changes = defaultdict(list)

    for result in results:
        if not result.confidence_trajectory or len(result.confidence_trajectory) < 2:
            continue

        # Get questions from conversation (skip first "user" message which is the initial claim)
        questions = []
        for i, entry in enumerate(result.conversation):
            if entry["role"] == "user" and i > 0:  # Skip initial claim
                questions.append(entry["content"])

        # Calculate confidence changes after each question
        for i, question in enumerate(questions):
            if i + 1 < len(result.confidence_trajectory):
                conf_before = result.confidence_trajectory[i] if i > 0 else 0.5
                conf_after = result.confidence_trajectory[i + 1]
                change = abs(conf_after - conf_before)

                # Categorize question
                category = categorize_single_question(question)
                category_changes[category].append(change)

    # Compute statistics per category
    stats = {}
    for category, changes in category_changes.items():
        if changes:
            stats[category] = {
                "count": len(changes),
                "avg_change": np.mean(changes),
                "median_change": np.median(changes),
                "max_change": np.max(changes),
                "std_change": np.std(changes)
            }
        else:
            stats[category] = {
                "count": 0,
                "avg_change": 0,
                "median_change": 0,
                "max_change": 0,
                "std_change": 0
            }

    return stats


def categorize_single_question(question: str) -> str:
    """Categorize a single question."""
    categories = categorize_questions([question])
    # Return the category with count > 0
    for cat, count in categories.items():
        if count > 0:
            return cat
    return "other"


def analyze_failure_cases(
    results: List[InterrogationResult],
    ground_truth: List[bool]  # True for lying, False for truthful
) -> Dict:
    """
    Analyze patterns in failure cases.

    Args:
        results: List of interrogation results
        ground_truth: List of ground truth labels (True=lying, False=truthful)

    Returns:
        Dictionary with failure analysis
    """
    failures = []
    successes = []

    for result, is_lying in zip(results, ground_truth):
        correct = (is_lying and result.final_prediction == "lying") or \
                 (not is_lying and result.final_prediction == "truthful")

        case = {
            "result": result,
            "ground_truth": "lying" if is_lying else "truthful",
            "correct": correct
        }

        if correct:
            successes.append(case)
        else:
            failures.append(case)

    # Analyze failure patterns
    analysis = {
        "total_cases": len(results),
        "failures": len(failures),
        "successes": len(successes),
        "failure_rate": len(failures) / len(results) if results else 0,

        "failure_patterns": {
            "false_positives": 0,  # Predicted lying when truthful
            "false_negatives": 0,  # Predicted truthful when lying
            "uncertain": 0,  # Predicted uncertain
            "avg_questions_failures": 0,
            "avg_questions_successes": 0,
            "avg_confidence_failures": 0,
            "avg_confidence_successes": 0,
        }
    }

    # Analyze failure patterns
    for case in failures:
        result = case["result"]

        if result.final_prediction == "lying" and case["ground_truth"] == "truthful":
            analysis["failure_patterns"]["false_positives"] += 1
        elif result.final_prediction == "truthful" and case["ground_truth"] == "lying":
            analysis["failure_patterns"]["false_negatives"] += 1
        elif result.final_prediction == "uncertain":
            analysis["failure_patterns"]["uncertain"] += 1

    # Average questions and confidence
    if failures:
        analysis["failure_patterns"]["avg_questions_failures"] = \
            np.mean([c["result"].questions_asked for c in failures])
        analysis["failure_patterns"]["avg_confidence_failures"] = \
            np.mean([c["result"].final_confidence for c in failures])

    if successes:
        analysis["failure_patterns"]["avg_questions_successes"] = \
            np.mean([c["result"].questions_asked for c in successes])
        analysis["failure_patterns"]["avg_confidence_successes"] = \
            np.mean([c["result"].final_confidence for c in successes])

    # Store actual failure cases for inspection
    analysis["failure_cases"] = failures

    return analysis


def print_question_analysis(stats: Dict):
    """Print question type analysis in readable format."""
    print("\n" + "=" * 80)
    print("QUESTION TYPE ANALYSIS")
    print("=" * 80)

    print("\nðŸ“Š Diagnostic Value by Question Type:")
    print("-" * 80)

    # Sort by average change (most diagnostic first)
    sorted_types = sorted(stats.items(),
                         key=lambda x: x[1]["avg_change"],
                         reverse=True)

    for i, (category, metrics) in enumerate(sorted_types, 1):
        if metrics["count"] > 0:
            print(f"\n{i}. {category.replace('_', ' ').title()}")
            print(f"   Count:        {metrics['count']}")
            print(f"   Avg change:   {metrics['avg_change']:.4f}")
            print(f"   Median:       {metrics['median_change']:.4f}")
            print(f"   Max change:   {metrics['max_change']:.4f}")
            print(f"   Std dev:      {metrics['std_change']:.4f}")

            # Visual bar
            bar_length = int(metrics['avg_change'] * 100)
            bar = "â–ˆ" * min(bar_length, 50)
            print(f"   Impact:       {bar}")


def print_failure_analysis(analysis: Dict):
    """Print failure case analysis in readable format."""
    print("\n" + "=" * 80)
    print("FAILURE CASE ANALYSIS")
    print("=" * 80)

    print(f"\nðŸ“Š Overall Performance:")
    print(f"  Total cases:      {analysis['total_cases']}")
    print(f"  Successes:        {analysis['successes']} ({analysis['successes']/analysis['total_cases']*100:.1f}%)")
    print(f"  Failures:         {analysis['failures']} ({analysis['failure_rate']*100:.1f}%)")

    if analysis['failures'] > 0:
        print(f"\nâŒ Failure Breakdown:")
        fp = analysis['failure_patterns']['false_positives']
        fn = analysis['failure_patterns']['false_negatives']
        unc = analysis['failure_patterns']['uncertain']

        print(f"  False positives:  {fp} ({fp/analysis['failures']*100:.1f}%)")
        print(f"  False negatives:  {fn} ({fn/analysis['failures']*100:.1f}%)")
        print(f"  Uncertain:        {unc} ({unc/analysis['failures']*100:.1f}%)")

        print(f"\nðŸ“ˆ Comparison:")
        print(f"  Avg questions (failures):  {analysis['failure_patterns']['avg_questions_failures']:.2f}")
        print(f"  Avg questions (successes): {analysis['failure_patterns']['avg_questions_successes']:.2f}")
        print(f"  Avg confidence (failures):  {analysis['failure_patterns']['avg_confidence_failures']:.3f}")
        print(f"  Avg confidence (successes): {analysis['failure_patterns']['avg_confidence_successes']:.3f}")

        print(f"\nðŸ” Insights:")
        if analysis['failure_patterns']['avg_questions_failures'] > \
           analysis['failure_patterns']['avg_questions_successes']:
            print("  â€¢ Failures take MORE questions (system struggling)")
        else:
            print("  â€¢ Failures take FEWER questions (system stopping too early?)")

        if analysis['failure_patterns']['avg_confidence_failures'] > 0.6:
            print("  â€¢ Failures have high confidence (overconfident errors)")
        else:
            print("  â€¢ Failures have low confidence (uncertain predictions)")
    else:
        print("\nâœ… No failures detected!")


def plot_question_analysis(stats: Dict, output_path: str):
    """Plot question type diagnostic value."""
    if not MATPLOTLIB_AVAILABLE:
        print("Warning: matplotlib not available, skipping plots")
        return

    # Filter out categories with no data
    filtered_stats = {k: v for k, v in stats.items() if v["count"] > 0}

    if not filtered_stats:
        print("Warning: No question data to plot")
        return

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    fig.suptitle('Question Type Analysis', fontsize=16, fontweight='bold')

    # Sort by average change
    sorted_items = sorted(filtered_stats.items(),
                         key=lambda x: x[1]["avg_change"],
                         reverse=True)
    categories = [item[0].replace('_', ' ').title() for item in sorted_items]
    avg_changes = [item[1]["avg_change"] for item in sorted_items]
    counts = [item[1]["count"] for item in sorted_items]

    # Plot 1: Average confidence change by question type
    ax1.barh(categories, avg_changes, color='steelblue', alpha=0.8)
    ax1.set_xlabel('Avg Confidence Change', fontsize=12)
    ax1.set_title('Diagnostic Value by Question Type', fontsize=14, fontweight='bold')
    ax1.grid(axis='x', alpha=0.3)

    # Add value labels
    for i, v in enumerate(avg_changes):
        ax1.text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=10)

    # Plot 2: Question type frequency
    ax2.barh(categories, counts, color='coral', alpha=0.8)
    ax2.set_xlabel('Count', fontsize=12)
    ax2.set_title('Question Type Frequency', fontsize=14, fontweight='bold')
    ax2.grid(axis='x', alpha=0.3)

    # Add value labels
    for i, v in enumerate(counts):
        ax2.text(v + 0.1, i, str(v), va='center', fontsize=10)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nðŸ“Š Question analysis plot saved to: {output_path}")
    plt.close()


if __name__ == "__main__":
    # Demo
    from src.adaptive_system import create_adaptive_detector
    from src.data_generator import MockTargetModel

    print("Running question analysis demo...")

    # Create detector
    detector = create_adaptive_detector(
        classifier_path="data/results/trained_classifier.pkl",
        confidence_threshold=0.8,
        max_questions=8,
        use_mock=True
    )

    # Create test claims with ground truth
    test_data = [
        ("I visited Paris last summer", False),  # truthful
        ("I am an expert in quantum physics", True),  # lying
        ("Water boils at 100 degrees Celsius", False),  # truthful
        ("I won a Nobel Prize in Chemistry", True),  # lying
        ("The Earth orbits the Sun", False),  # truthful
        ("I speak 20 languages fluently", True),  # lying
        ("Python is a programming language", False),  # truthful
        ("I climbed Mount Everest last year", True),  # lying
    ]

    # Run interrogations
    target = MockTargetModel()
    results = []
    ground_truth = []

    print("\nRunning interrogations...")
    for claim, is_lying in test_data:
        if is_lying:
            target.set_mode("lie")
        else:
            target.set_mode("truth")

        result = detector.interrogate(target, claim, verbose=False)
        results.append(result)
        ground_truth.append(is_lying)

        if hasattr(target, 'reset_conversation'):
            target.reset_conversation()

    # Analyze question types
    print("\n" + "=" * 80)
    print("ANALYZING QUESTION TYPES")
    print("=" * 80)

    # Collect all questions (skip first "user" message which is the initial claim)
    all_questions = []
    for result in results:
        for i, entry in enumerate(result.conversation):
            if entry["role"] == "user" and i > 0:  # Skip initial claim
                all_questions.append(entry["content"])

    categories = categorize_questions(all_questions)
    print(f"\nTotal questions: {len(all_questions)}")
    print("\nQuestion type distribution:")
    for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
        pct = count / len(all_questions) * 100 if all_questions else 0
        print(f"  {cat.replace('_', ' ').title():25s}: {count:3d} ({pct:5.1f}%)")

    # Analyze diagnostic value
    print("\n" + "=" * 80)
    print("ANALYZING DIAGNOSTIC VALUE")
    print("=" * 80)

    diagnostic_stats = analyze_diagnostic_value(results)
    print_question_analysis(diagnostic_stats)

    # Analyze failures
    print("\n" + "=" * 80)
    print("ANALYZING FAILURES")
    print("=" * 80)

    failure_analysis = analyze_failure_cases(results, ground_truth)
    print_failure_analysis(failure_analysis)

    # Generate plots
    from src.utils import timestamp
    plot_question_analysis(
        diagnostic_stats,
        f"data/results/question_analysis_{timestamp()}.png"
    )
